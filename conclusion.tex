\chapter*{Conclusion \TO}				   % DO NOT TOUCH!
\addcontentsline{toc}{chapter}{Conclusion \TO} % DO NOT TOUCH!

The objective of this project was to study, implement, and compare the performance of the GPU implementation of LU decomposition to the CPU implementation.
\par First, the comparison of recent CPUs and GPUs was presented in order to lay a foundation of advantages and disadvantages to using either. Then, the nuances of the software layer used to orchestrate the execution of code on the GPU was presented - from CUDA's thread and memory management systems, to its sophisticated concurrent execution solution and examples that tied together the theory presented. Furthermore, the LU decomposition method was shown in both its direct and iterative versions.
\par Following the theory, the TNL project - which provided a data structure paramount to the development - was briefly described. Taking the theory and the dependencies into account, subsequently, the project that contained the implementation of the LU decomposition in its CPU and GPU form was detailed. In addition to the methods' implementations, unit tests assuring their quality and benchmarks measuring their performance were added to the project. Furthermore, the evolution of the GPU's LU decomposition implementation was expounded.
\par Finally, the speed and accuracy of the CPU and GPU implementations were compared using the benchmark structure incorporated in the project. Specifically, the process of measuring their performance consisted of decomposing a curated set of 63 matrices with varying characteristics on a state-of-the-art compute cluster. While the results of the benchmarks indicated that the GPU version of LU decomposition with 32 threads per block was the optimal choice in general, other implementations were, in specific cases, found to be more suitable. Overall, the performance of the CPU version was sub-optimal compared to that of the GPU version, with the following exceptions: decomposing matrices smaller than $ 500\times 500 $ and dense matrices using double precision - the latter instance of outperformance was only present when the CPU version was compared to the 8-thread-per-block GPU version.
\par In terms of future work, as mentioned in \textit{\nameref{Section:comparing-decomposition-implementations-benchmark-results}} in Section~\ref{Section:comparing-decomposition-implementations-benchmark-results} the benchmarks were run on a set of only 63 matrices. While this number is sufficient for a preliminary comparison of implementations it is not enough to make definitive claims on their overall performance. Furthermore, notwithstanding the promising results, it is necessary to compare the performance of both the CPU and GPU versions to a well-established solution on real-life problems. Moreover, in addition to detailing the evolution of the GPU implementation, Section~\ref{Section:implementation-optimization} \textit{\nameref{Section:implementation-optimization}} mentions possible improvements that may greatly increase its performance. In summary, future work includes both the extension of benchmarks and further optimizations of the implementations.